{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"My_Sigmoid_Neuron.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"00XYeUm79_7e","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NwrzYkv3-T5v","colab_type":"text"},"cell_type":"markdown","source":["## Model\n","\n","$$y = 1 / (1+e^{-{w^TX+b}}) $$ "]},{"metadata":{"id":"lMplhtgL-P-c","colab_type":"code","colab":{}},"cell_type":"code","source":["# we are given a dataset with 90 items with 5 columns for each\n","wt = np.random.rand(5,1)\n","X = np.random.rand(90,5)\n","Y = np.random.rand(90,1)\n","b = -0.5\n","\n","def sigmoid_neuron(w,b,x):\n","  Y_pred = 1/(1+np.exp(-(np.dot(x,wt) + b)))\n","  return Y_pred\n","\n","Y_pred = sigmoid_neuron(wt,b,X[0,:])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RYlQ2M83AzCR","colab_type":"text"},"cell_type":"markdown","source":["## Loss Function\n","\n","$$ Loss = \\sum_i^n (y_i^{pred}-y_i^{actual})^2$$"]},{"metadata":{"id":"KnLXqu6cBpUI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dc87c605-43fb-4be7-d2ce-f1d6d5e92ddc","executionInfo":{"status":"ok","timestamp":1551154833147,"user_tz":-330,"elapsed":861,"user":{"displayName":"Kavyajeet Bora","photoUrl":"https://lh6.googleusercontent.com/-ODtK_cL-VT8/AAAAAAAAAAI/AAAAAAAAH_s/sxbvyeIxZsg/s64/photo.jpg","userId":"17684843864183035480"}}},"cell_type":"code","source":["def loss_function(Y_pred,Y):\n","  return np.sum(np.power((Y_pred-Y),2))\n","\n","loss_function(Y_pred,Y)"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.217312004907198"]},"metadata":{"tags":[]},"execution_count":26}]},{"metadata":{"id":"Hx23pvp4DFX2","colab_type":"text"},"cell_type":"markdown","source":["## Graient descent (Learning) - Parameter Update \n","\n","\n","$$w_{t+1} = w_t - \\eta\\Delta w_t$$\n","\n","$$b_{t+1} = b_t - \\eta\\Delta b_t$$\n","\n","where\n","\n","$$\\Delta w_t = \\frac{\\partial L(w,b)} {\\partial w},  \\Delta b_t = \\frac{\\partial L(w,b)} {\\partial b}, \\eta = step$$\n","\n","Derivatives are given by - \n","\n","$$\\Delta w_t = \\sum_i^n (f(x_i)-y_i)(f(x_i))(1-f(x_i))x_i $$\n"," $$\\Delta b_t = \\sum_i^n (f(x_i)-y_i)(f(x_i))(1-f(x_i)) $$\n","\n"]},{"metadata":{"id":"KAXCyy76DEVW","colab_type":"code","colab":{}},"cell_type":"code","source":["def grad_w(w,b,x,y):\n","  f_x =  sigmoid_neuron(w,b,x)\n","  return (f_x-y)*(f_x)*(1-f_x)*x\n","\n","def grad_b(w,b,x,y):\n","  f_x =  sigmoid_neuron(w,b,x)\n","  return (f_x-y)*(f_x)*(1-f_x)*x\n","\n","def do_gradient_descent(eta=0.1,max_epochs=100):\n","  w,b = 0,0\n","  dw,db = 0,0\n","  for x,y in zip(X,Y):\n","    dw += grad_w(w,b,x,y)\n","    db += grad_b(w,b,x,y)\n","  w = w - eta*dw\n","  b = b - eta*db"],"execution_count":0,"outputs":[]}]}