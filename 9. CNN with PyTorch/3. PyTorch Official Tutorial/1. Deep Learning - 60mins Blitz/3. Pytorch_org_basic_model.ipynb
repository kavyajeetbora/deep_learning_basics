{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_org_tutorials_2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"u6Kusmn0kp3i","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lh9W1APXk-fd","colab_type":"text"},"cell_type":"markdown","source":["https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py"]},{"metadata":{"id":"ECvErG_Xl0Ak","colab_type":"text"},"cell_type":"markdown","source":["## 1. Neural Network Class with Pytorch"]},{"metadata":{"id":"LgI0ei_pl9CM","colab_type":"text"},"cell_type":"markdown","source":["**Convolution operation**\n","\n","- Input = black and white image with 32 by 32 pixels\n","- Layer 1 = conv2d with 6 filters with kernel = 5,5\n","- Layer 2 = max_pooling with kernel 2,2\n","- Layer 3 = conv2d with 16 filters with kernel (5,5)\n","- Layer 4 = max_pooling with kernel 2,2\n","\n","**Fully Connected Neural Network**\n","\n"," - Flatten the output from convolution operation\n"," - Feed forward to a 120 fully connected neurons\n"," - Feed forward to 82 neurons\n"," - output to 10 neurons\n","\n"]},{"metadata":{"id":"4mD_bUrKkyul","colab_type":"code","colab":{}},"cell_type":"code","source":["class LeNet(nn.Module):\n","  \n","  def __init__(self):\n","    \n","    super(LeNet,self).__init__()\n","    self.conv1 = nn.Conv2d(1,6,5)   # (N,1,32,32) -> (N,6,28,28)\n","    self.conv2 = nn.Conv2d(6,16,(5,5)) # (N,6,14,14) -> (N,16,10,10)\n","    \n","    # Full Connected\n","    self.fc1 = nn.Linear(400,120)\n","    self.fc2 = nn.Linear(120,84)\n","    self.fc3 = nn.Linear(84,10)\n","    \n","  def forward(self,X):\n","    x = F.max_pool2d(F.relu(self.conv1(X)),(2,2))\n","    x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n","    # flatten the output from convolution operation\n","    x = x.view(x.size(0),-1)\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lWbGtR09td5Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"30d992b0-1527-4247-c9d0-38340a479c88","executionInfo":{"status":"ok","timestamp":1556605385573,"user_tz":-330,"elapsed":1351,"user":{"displayName":"Kavyajeet Bora","photoUrl":"https://lh6.googleusercontent.com/-ODtK_cL-VT8/AAAAAAAAAAI/AAAAAAAAH_s/sxbvyeIxZsg/s64/photo.jpg","userId":"17684843864183035480"}}},"cell_type":"code","source":["net = LeNet()\n","print(net)"],"execution_count":99,"outputs":[{"output_type":"stream","text":["LeNet(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"qrCtZ8zAyFCK","colab_type":"text"},"cell_type":"markdown","source":["## Backpropagation and Weight Update"]},{"metadata":{"id":"ku_V8tO76tAh","colab_type":"text"},"cell_type":"markdown","source":["### Backpropagation"]},{"metadata":{"id":"Kp9E4bbgtjTR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"d01b8187-9c6f-46b0-caea-d98062c71666","executionInfo":{"status":"ok","timestamp":1556605385574,"user_tz":-330,"elapsed":1345,"user":{"displayName":"Kavyajeet Bora","photoUrl":"https://lh6.googleusercontent.com/-ODtK_cL-VT8/AAAAAAAAAAI/AAAAAAAAH_s/sxbvyeIxZsg/s64/photo.jpg","userId":"17684843864183035480"}}},"cell_type":"code","source":["params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())"],"execution_count":100,"outputs":[{"output_type":"stream","text":["10\n","torch.Size([6, 1, 5, 5])\n"],"name":"stdout"}]},{"metadata":{"id":"TKv9qbnktvUY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"9315ff93-0f52-443e-bd25-5b85a89b10a5","executionInfo":{"status":"ok","timestamp":1556605385575,"user_tz":-330,"elapsed":1337,"user":{"displayName":"Kavyajeet Bora","photoUrl":"https://lh6.googleusercontent.com/-ODtK_cL-VT8/AAAAAAAAAAI/AAAAAAAAH_s/sxbvyeIxZsg/s64/photo.jpg","userId":"17684843864183035480"}}},"cell_type":"code","source":["# try a random input\n","x_input = torch.randn(1,1,32,32)\n","out = net(x_input)\n","print(out)"],"execution_count":101,"outputs":[{"output_type":"stream","text":["tensor([[-0.0100,  0.0623, -0.0195, -0.0446,  0.0623,  0.1433,  0.0183, -0.0838,\n","          0.1426, -0.1487]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"pOjob-xjvBH7","colab_type":"code","colab":{}},"cell_type":"code","source":["# computing the loss\n","loss_fn = nn.MSELoss()\n","true = torch.randn(1,10)\n","loss = loss_fn(out,true)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VP8UcM8V42CR","colab_type":"text"},"cell_type":"markdown","source":["So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient"]},{"metadata":{"id":"DNmlpjjoupUq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"72e4c5c4-d956-4b9f-9421-731a9716d9ab","executionInfo":{"status":"ok","timestamp":1556605385579,"user_tz":-330,"elapsed":1328,"user":{"displayName":"Kavyajeet Bora","photoUrl":"https://lh6.googleusercontent.com/-ODtK_cL-VT8/AAAAAAAAAAI/AAAAAAAAH_s/sxbvyeIxZsg/s64/photo.jpg","userId":"17684843864183035480"}}},"cell_type":"code","source":["print(loss.grad_fn)\n","print(loss.grad_fn.next_functions[0][0])\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"],"execution_count":103,"outputs":[{"output_type":"stream","text":["<MseLossBackward object at 0x7f90796cf470>\n","<AddmmBackward object at 0x7f90796cf4a8>\n","<AccumulateGrad object at 0x7f90796cf470>\n"],"name":"stdout"}]},{"metadata":{"id":"xzAErkox5FFa","colab_type":"text"},"cell_type":"markdown","source":["**To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.**"]},{"metadata":{"id":"VFogxRon1hrT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"a3158fee-a054-45e3-8bee-ae061cd868d2","executionInfo":{"status":"ok","timestamp":1556605396139,"user_tz":-330,"elapsed":1376,"user":{"displayName":"Kavyajeet Bora","photoUrl":"https://lh6.googleusercontent.com/-ODtK_cL-VT8/AAAAAAAAAAI/AAAAAAAAH_s/sxbvyeIxZsg/s64/photo.jpg","userId":"17684843864183035480"}}},"cell_type":"code","source":["net.zero_grad()\n","\n","print('conv1.bias.grad before backward function')\n","print(net.conv1.bias.grad)\n","\n","loss.backward(retain_graph=True)\n","print('conv1.bias.grad after backward function')\n","print(net.conv1.bias.grad)"],"execution_count":106,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward function\n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward function\n","tensor([ 0.0210,  0.0023,  0.0020, -0.0207, -0.0004,  0.0015])\n"],"name":"stdout"}]},{"metadata":{"id":"cLedeSD66Guy","colab_type":"text"},"cell_type":"markdown","source":["**To reduce memory usage, during the .backward() call, all the intermediary results are deleted** when they are not needed anymore. Hence if you try to call .backward() again, the intermediary results donâ€™t exist and the backward pass cannot be performed (and you get the error you see).\n","You can call .backward(retain_graph=True) to make a backward pass that will not delete intermediary results, and so you will be able to call .backward() again. All but the last call to backward should have the retain_graph=True option.\n","\n","*For more Information:* https://discuss.pytorch.org/t/runtimeerror-trying-to-backward-through-the-graph-a-second-time-but-the-buffers-have-already-been-freed-specify-retain-graph-true-when-calling-backward-the-first-time/6795"]},{"metadata":{"id":"8LjChsfx6qDS","colab_type":"text"},"cell_type":"markdown","source":["### Updating the Weights"]},{"metadata":{"id":"26l4GCFh55fJ","colab_type":"code","colab":{}},"cell_type":"code","source":["opt = optim.SGD(net.parameters(),lr=0.01)\n","opt.zero_grad() # zero the gradient buffers\n","output = net(x_input)\n","loss = loss_fn(output,true)\n","loss.backward()\n","opt.step()  # step updates the weights"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wQaohrUN8C2X","colab_type":"text"},"cell_type":"markdown","source":["**The use of zero_grad in pytorch**\n","\n","You need to clear the existing gradients though, else gradients will be accumulated to existing gradients\n","\n","*for more information: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html*"]}]}